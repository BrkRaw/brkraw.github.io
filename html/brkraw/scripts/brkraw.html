<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.1" />
<title>brkraw.scripts.brkraw API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>brkraw.scripts.brkraw</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># -*- coding: utf-8 -*-
from shleeh import *
from shleeh.errors import *
from .. import BrukerLoader, __version__
import argparse
import os, re

_supporting_bids_ver = &#39;1.2.2&#39;


def mkdir(path):
    try:
        os.stat(path)
    except FileNotFoundError or OSError:
        os.makedirs(path)
    except:
        raise UnexpectedError


def main():
    parser = argparse.ArgumentParser(prog=&#39;brkraw&#39;,
                                     description=&#34;BrkRaw command-line interface&#34;)
    parser.add_argument(&#34;-v&#34;, &#34;--version&#34;, action=&#39;version&#39;, version=&#39;%(prog)s v{}&#39;.format(__version__))

    subparsers = parser.add_subparsers(title=&#39;Sub-commands&#39;,
                                       description=&#39;To run this command, you must specify one of the functions listed&#39;
                                                   &#39;below next to the command. For more information on each function, &#39;
                                                   &#39;use -h next to the function name to call help document.&#39;,
                                       help=&#39;description&#39;,
                                       dest=&#39;function&#39;,
                                       metavar=&#39;command&#39;)

    input_str = &#34;input raw Bruker data&#34;
    input_dir_str = &#34;input directory that contains multiple raw Bruker data&#34;
    output_dir_str = &#34;output directory name&#34;
    output_fnm_str = &#34;output filename&#34;
    bids_opt = &#34;create a JSON file contains metadata according to BIDS recommendation&#34;

    info = subparsers.add_parser(&#34;info&#34;, help=&#39;Prints out the information of the internal contents in Bruker raw data&#39;)
    info.add_argument(&#34;input&#34;, help=input_str, type=str)

    gui = subparsers.add_parser(&#34;gui&#34;, help=&#39;Run GUI mode&#39;)
    nii = subparsers.add_parser(&#34;tonii&#34;, help=&#39;Convert a single raw Bruker data into NifTi file(s)&#39;)
    niiall = subparsers.add_parser(&#34;tonii_all&#34;, help=&#34;Convert All raw Bruker data located in the input directory&#34;)
    bids_helper = subparsers.add_parser(&#34;bids_helper&#34;, help=&#34;Creates a BIDS datasheet &#34;
                                                            &#34;for guiding BIDS data converting.&#34;)
    bids_convert = subparsers.add_parser(&#34;bids_convert&#34;, help=&#34;Convert ALL raw Bruker data located &#34;
                                                              &#34;in the input directory based on the BIDS datasheet&#34;)

    gui.add_argument(&#34;-i&#34;, &#34;--input&#34;, help=input_str, type=str, default=None)
    gui.add_argument(&#34;-o&#34;, &#34;--output&#34;, help=output_dir_str, type=str, default=None)

    nii.add_argument(&#34;input&#34;, help=input_str, type=str)
    nii.add_argument(&#34;-b&#34;, &#34;--bids&#34;, help=bids_opt, action=&#39;store_true&#39;)
    nii.add_argument(&#34;-o&#34;, &#34;--output&#34;, help=output_fnm_str, type=str, default=False)
    nii.add_argument(&#34;-r&#34;, &#34;--recoid&#34;, help=&#34;RECO ID&#34;, type=int, default=1)
    nii.add_argument(&#34;-s&#34;, &#34;--scanid&#34;, help=&#34;Scan ID&#34;, type=str)

    niiall.add_argument(&#34;input&#34;, help=input_dir_str, type=str)
    niiall.add_argument(&#34;-o&#34;, &#34;--output&#34;, help=output_dir_str, type=str)
    niiall.add_argument(&#34;-b&#34;, &#34;--bids&#34;, help=bids_opt, action=&#39;store_true&#39;)

    bids_helper.add_argument(&#34;input&#34;, help=input_dir_str, type=str)
    bids_helper.add_argument(&#34;output&#34;, help=&#34;output BIDS datasheet filename (.xlsx)&#34;, type=str)
    bids_helper.add_argument(&#34;-j&#34;, &#34;--json&#34;, help=&#34;create JSON syntax template for &#34;
                                                  &#34;parsing metadata from the header&#34;, action=&#39;store_true&#39;)

    bids_convert.add_argument(&#34;input&#34;, help=input_dir_str, type=str)
    bids_convert.add_argument(&#34;datasheet&#34;, help=&#34;input BIDS datahseet filename&#34;, type=str)
    bids_convert.add_argument(&#34;-j&#34;, &#34;--json&#34;, help=&#34;input JSON syntax template filename&#34;, type=str, default=False)
    bids_convert.add_argument(&#34;-o&#34;, &#34;--output&#34;, help=output_dir_str, type=str, default=False)

    args = parser.parse_args()

    if args.function == &#39;info&#39;:
        path = args.input
        if any([os.path.isdir(path), (&#39;zip&#39; in path), (&#39;PvDataset&#39; in path)]):
            study = BrukerLoader(path)
            study.info()
        else:
            list_path = [d for d in os.listdir(&#39;.&#39;) if (any([os.path.isdir(d),
                                                             (&#39;zip&#39; in d),
                                                             (&#39;PvDataset&#39; in d)]) and re.search(path, d, re.IGNORECASE))]
            for p in list_path:
                study = BrukerLoader(p)
                study.info()

    elif args.function == &#39;gui&#39;:
        ipath = args.input
        opath = args.output
        from ..ui.main_win import MainWindow
        root = MainWindow()
        if ipath != None:
            root._path = ipath
            root._extend_layout()
            root._load_dataset()
        if opath != None:
            root._output = opath
        root.mainloop()

    elif args.function == &#39;tonii&#39;:
        path = args.input
        scan_id = args.scanid
        reco_id = args.recoid
        study = BrukerLoader(path)
        if args.output:
            output = args.output
        else:
            output = &#39;{}_{}&#39;.format(study._pvobj.subj_id,study._pvobj.study_id)
        if scan_id:
            output_fname = &#39;{}-{}-{}&#39;.format(output, scan_id, reco_id)
            try:
                study.save_as(scan_id, reco_id, output_fname)
                if args.bids:
                    study.save_json(scan_id, reco_id, output_fname)
                print(&#39;NifTi file is generated... [{}]&#39;.format(output_fname))
            except Exception as e:
                print(&#39;[Warning]::{}&#39;.format(e))
        else:
            for scan_id, recos in study._pvobj.avail_reco_id.items():
                for reco_id in recos:
                    output_fname = &#39;{}-{}-{}&#39;.format(output, str(scan_id).zfill(2), reco_id)
                    try:
                        study.save_as(scan_id, reco_id, output_fname)
                        if args.bids:
                            study.save_json(scan_id, reco_id, output_fname)
                        print(&#39;NifTi file is genetared... [{}]&#39;.format(output_fname))
                    except Exception as e:
                        print(&#39;[Warning]::{}&#39;.format(e))

    elif args.function == &#39;tonii_all&#39;:
        path = args.input
        from os.path import join as opj, isdir, isfile
        list_of_raw = sorted([d for d in os.listdir(path) if isdir(opj(path, d)) \
                              or (isfile(opj(path, d)) and ((&#39;zip&#39; in d) or (&#39;PvDataset&#39; in d)))])
        base_path = &#39;Data&#39;
        try:
            os.mkdir(base_path)
        except:
            pass
        for raw in list_of_raw:
            sub_path = os.path.join(path, raw)
            study = BrukerLoader(sub_path)
            if len(study._pvobj.avail_scan_id):
                subj_path = os.path.join(base_path, &#39;sub-{}&#39;.format(study._pvobj.subj_id))
                try:
                    os.mkdir(subj_path)
                except OSError:
                    pass
                else:
                    raise UnexpectedError
                sess_path = os.path.join(subj_path, &#39;ses-{}&#39;.format(study._pvobj.study_id))
                try:
                    os.mkdir(sess_path)
                except OSError:
                    pass
                else:
                    raise UnexpectedError
                for scan_id, recos in study._pvobj.avail_reco_id.items():
                    method = study._pvobj._method[scan_id].parameters[&#39;Method&#39;]
                    if re.search(&#39;epi&#39;, method, re.IGNORECASE) and not re.search(&#39;dti&#39;, method, re.IGNORECASE):
                        output_path = os.path.join(sess_path, &#39;func&#39;)
                    elif re.search(&#39;dti&#39;, method, re.IGNORECASE):
                        output_path = os.path.join(sess_path, &#39;dwi&#39;)
                    elif re.search(&#39;flash&#39;, method, re.IGNORECASE) or re.search(&#39;rare&#39;, method, re.IGNORECASE):
                        output_path = os.path.join(sess_path, &#39;anat&#39;)
                    else:
                        output_path = os.path.join(sess_path, &#39;etc&#39;)
                    try:
                        os.mkdir(output_path)
                    except OSError:
                        pass
                    else:
                        raise UnexpectedError
                    filename = &#39;sub-{}_ses-{}_{}&#39;.format(study._pvobj.subj_id, study._pvobj.study_id,
                                                         str(scan_id).zfill(2))
                    for reco_id in recos:
                        output_fname = os.path.join(output_path, &#39;{}_reco-{}&#39;.format(filename,
                                                                                     str(reco_id).zfill(2)))
                        try:
                            study.save_as(scan_id, reco_id, output_fname)
                            if args.bids:
                                study.save_json(scan_id, reco_id, output_fname)
                            if re.search(&#39;dti&#39;, method, re.IGNORECASE):
                                study.save_bdata(scan_id, reco_id, output_fname)
                        except Exception as e:
                            print(e)
                print(f&#39;{raw} is converted...&#39;)
            else:
                print(f&#39;{raw} is empty...&#39;)

    elif args.function == &#39;bids_helper&#39;:
        import pandas as pd
        path = os.path.abspath(args.input)
        ds_output = os.path.abspath(args.output)

        make_json = args.json
        if not ds_output.endswith(&#39;.xlsx&#39;):
            # to prevent pandas ValueError in case user does not provide valid file extension.
            output = f&#39;{ds_output}.xlsx&#39;
        else:
            output = ds_output

        Headers = [&#39;RawData&#39;, &#39;SubjID&#39;, &#39;SessID&#39;, &#39;ScanID&#39;, &#39;RecoID&#39;, &#39;DataType&#39;,
                   &#39;task&#39;, &#39;acq&#39;, &#39;ce&#39;, &#39;rec&#39;, &#39;dir&#39;, &#39;run&#39;, &#39;modality&#39;, &#39;Start&#39;, &#39;End&#39;]
        df = pd.DataFrame(columns=Headers)

        for dname in sorted(os.listdir(path)):
            dpath = os.path.join(path, dname)
            try:
                dset = BrukerLoader(dpath)
            except:
                dset = None

            if dset is not None:
                if dset.is_pvdataset:
                    pvobj = dset.pvobj

                    rawdata = pvobj.path
                    subj_id = pvobj.subj_id
                    sess_id = pvobj.session_id

                    for scan_id, recos in pvobj.avail_reco_id.items():
                        for reco_id in recos:
                            visu_pars = dset.get_visu_pars(scan_id, reco_id)
                            if dset._get_dim_info(visu_pars)[1] == &#39;spatial_only&#39;:
                                num_spack = dset._get_slice_info(visu_pars)[&#39;num_slice_packs&#39;]

                                if num_spack != 3:  # excluding localizer
                                    method = dset.get_method(scan_id).parameters[&#39;Method&#39;]
                                    if re.search(&#39;epi&#39;, method, re.IGNORECASE) and not re.search(&#39;dti&#39;, method, re.IGNORECASE):
                                        datatype = &#39;func&#39;
                                    elif re.search(&#39;dti&#39;, method, re.IGNORECASE):
                                        datatype = &#39;dwi&#39;
                                    elif re.search(&#39;flash&#39;, method, re.IGNORECASE) or re.search(&#39;rare&#39;, method, re.IGNORECASE):
                                        datatype = &#39;anat&#39;
                                    elif re.search(&#39;fieldmap&#39;, method, re.IGNORECASE):
                                        datatype = &#39;fmap&#39;
                                    else:
                                        datatype = &#39;etc&#39;

                                    item = dict(zip(Headers, [rawdata, subj_id, sess_id, scan_id, reco_id, datatype]))
                                    if datatype == &#39;fmap&#39;:
                                        for m, s, e in [[&#39;fieldmap&#39;, 0, 1], [&#39;magnitude&#39;, 1, 2]]:
                                            item[&#39;modality&#39;] = m
                                            item[&#39;Start&#39;] = s
                                            item[&#39;End&#39;] = e
                                            df = df.append(item, ignore_index=True)
                                    else:
                                        df = df.append(item, ignore_index=True)
        df.to_excel(output, index=None)

        if make_json:
            fname = output[:-5]
            json_fname = f&#39;{fname}.json&#39;
            print(f&#39;Creating JSON syntax template for parsing the BIDS required metadata &#39;
                  f&#39;(BIDS v{_supporting_bids_ver}): {json_fname}&#39;)
            with open(json_fname, &#39;w&#39;) as f:
                import json
                from ..lib.reference import COMMON_META_REF, FMRI_META_REF, FIELDMAP_META_REF
                ref_dict = dict(common=COMMON_META_REF,
                                func=FMRI_META_REF,
                                fmap=FIELDMAP_META_REF)
                json.dump(ref_dict, f, indent=4)

        print(&#39;[Important notice] The function helps to minimize the BIDS organization but does not guarantee that &#39;
              &#39;the dataset always meets the BIDS requirements. &#39;
              &#39;Therefore, after converting your data, we recommend validating &#39;
              &#39;your dataset using an official BIDS validator.&#39;)

    elif args.function == &#39;bids_convert&#39;:
        import pandas as pd
        import numpy as np
        import json
        import datetime
        from ..lib.utils import build_bids_json, bids_validation

        pd.options.mode.chained_assignment = None
        path = args.input
        datasheet = args.datasheet
        output = args.output
        df = pd.read_excel(datasheet, dtype={&#39;SubjID&#39;: str, &#39;SessID&#39;: str, &#39;run&#39;: str})
        json_fname = args.json

        # check if the project is multi-session
        if all(pd.isnull(df[&#39;SessID&#39;])):
            # SessID was removed
            multi_session = False
        else:
            num_session = len(list(set(df[&#39;SessID&#39;])))
            if num_session &gt; 1:
                multi_session = True
            else:
                multi_session = False

        if not output:
            root_path = os.path.abspath(os.path.join(os.path.curdir, &#39;Data&#39;))
        else:
            root_path = output

        mkdir(root_path)

        # prepare the required file for converted BIDS dataset
        data_des = &#39;dataset_description.json&#39;
        readme = &#39;README&#39;
        if not os.path.exists(data_des):
            with open(os.path.join(root_path, &#39;dataset_description.json&#39;), &#39;w&#39;) as f:
                from ..lib.reference import DATASET_DESC_REF
                json.dump(DATASET_DESC_REF, f, indent=4)
        if not os.path.exists(readme):
            with open(os.path.join(root_path, readme), &#39;w&#39;) as f:
                f.write(f&#39;This dataset has been converted using BrkRaw (v{__version__}) at {datetime.datetime.now()}.\n&#39;)
                f.write(&#39;## How to cite?\n - https://doi.org/10.5281/zenodo.3818615\n&#39;)

        print(&#39;Inspect input BIDS datasheet...&#39;)
        for dname in sorted(os.listdir(path)):
            dpath = os.path.join(path, dname)
            try:
                dset = BrukerLoader(dpath)
                if dset.is_pvdataset:
                    pvobj = dset.pvobj
                    rawdata = pvobj.path
                    filtered_dset = df[df[&#39;RawData&#39;].isin([rawdata])].reset_index()
                    filtered_dset.loc[:, &#39;FileName&#39;] = [np.nan] * len(filtered_dset)
                    filtered_dset.loc[:, &#39;Dir&#39;] = [np.nan] * len(filtered_dset)

                    if len(filtered_dset):
                        subj_id = list(set(filtered_dset[&#39;SubjID&#39;]))[0]
                        subj_code = f&#39;sub-{subj_id}&#39;

                        for i, row in filtered_dset.iterrows():
                            if multi_session:
                                # If multi-session, make session dir
                                sess_code = f&#39;ses-{row.SessID}&#39;
                                subj_path = os.path.join(root_path, subj_code)
                                mkdir(subj_path)
                                subj_path = os.path.join(subj_path, sess_code)
                                mkdir(subj_path)
                                # add session info to filename as well
                                fname = f&#39;{subj_code}_{sess_code}&#39;
                            else:
                                subj_path = os.path.join(root_path, subj_code)
                                mkdir(subj_path)
                                fname = f&#39;{subj_code}&#39;

                            datatype = row.DataType
                            dtype_path = os.path.join(subj_path, datatype)
                            mkdir(dtype_path)

                            if pd.notnull(row.task):
                                if bids_validation(df, i, &#39;task&#39;, row.task, 10):
                                    fname = f&#39;{fname}_task-{row.task}&#39;
                            if pd.notnull(row.acq):
                                if bids_validation(df, i, &#39;acq&#39;, row.acq, 10):
                                    fname = f&#39;{fname}_acq-{row.acq}&#39;
                            if pd.notnull(row.ce):
                                if bids_validation(df, i, &#39;ce&#39;, row.ce, 5):
                                    fname = f&#39;{fname}_ce-{row.ce}&#39;
                            if pd.notnull(row.dir):
                                if bids_validation(df, i, &#39;dir&#39;, row.dir, 2):
                                    fname = f&#39;{fname}_dir-{row.dir}&#39;
                            if pd.notnull(row.rec):
                                if bids_validation(df, i, &#39;rec&#39;, row.rec, 2):
                                    fname = f&#39;{fname}_rec-{row.rec}&#39;
                            filtered_dset.loc[i, &#39;FileName&#39;] = fname
                            filtered_dset.loc[i, &#39;Dir&#39;] = dtype_path
                            if pd.isnull(row.modality):
                                method = dset.get_method(row.ScanID).parameters[&#39;Method&#39;]
                                if row.DataType == &#39;anat&#39;:
                                    if re.search(&#39;flash&#39;, method, re.IGNORECASE):
                                        modality = &#39;FLASH&#39;
                                    elif re.search(&#39;rare&#39;, method, re.IGNORECASE):
                                        modality = &#39;T2w&#39;
                                    else:
                                        modality = &#39;{}&#39;.format(method.split(&#39;:&#39;)[-1])
                                else:
                                    modality = &#39;{}&#39;.format(method.split(&#39;:&#39;)[-1])
                                filtered_dset.loc[i, &#39;modality&#39;] = modality
                            else:
                                bids_validation(df, i, &#39;modality&#39;, row.modality, 10, dtype=str)

                        list_tested_fn = []
                        # Converting data according to the updated sheet
                        print(f&#39;Converting {dname}...&#39;)

                        for i, row in filtered_dset.iterrows():
                            temp_fname = f&#39;{row.FileName}_{row.modality}&#39;
                            if temp_fname not in list_tested_fn:
                                # filter the DataFrame that has same filename (updated without run)
                                fn_filter = filtered_dset.loc[:, &#39;FileName&#39;].isin([row.FileName])
                                fn_df = filtered_dset[fn_filter].reset_index(drop=True)

                                # filter specific modality from above DataFrame
                                md_filter = fn_df.loc[:, &#39;modality&#39;].isin([row.modality])
                                md_df = fn_df[md_filter].reset_index(drop=True)

                                if len(md_df) &gt; 1:
                                    conflict_tested = []
                                    for j, sub_row in md_df.iterrows():
                                        if pd.isnull(sub_row.run):
                                            fname = f&#39;{sub_row.FileName}_run-{str(j+1).zfill(2)}&#39;
                                        else:
                                            _ = bids_validation(df, i, &#39;run&#39;, sub_row.run, 3, dtype=int)
                                            fname = f&#39;{sub_row.FileName}_run-{str(sub_row.run).zfill(2)}&#39;
                                        if fname in conflict_tested:
                                            raise ValueConflictInField(f&#39;ScanID:[{sub_row.ScanID}] Conflict error. &#39;
                                                                       &#39;The [run] index value must be unique &#39;
                                                                       &#39;among the scans with the same modality.&#39;)
                                        else:
                                            conflict_tested.append(fname)
                                        build_bids_json(dset, sub_row, fname, json_fname)
                                else:
                                    fname = f&#39;{row.FileName}&#39;
                                    build_bids_json(dset, row, fname, json_fname)
                                list_tested_fn.append(temp_fname)
                        print(&#39;...Done.&#39;)
            except FileNotValidError:
                pass
    else:
        parser.print_help()


if __name__ == &#39;__main__&#39;:
    main()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="brkraw.scripts.brkraw.main"><code class="name flex">
<span>def <span class="ident">main</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def main():
    parser = argparse.ArgumentParser(prog=&#39;brkraw&#39;,
                                     description=&#34;BrkRaw command-line interface&#34;)
    parser.add_argument(&#34;-v&#34;, &#34;--version&#34;, action=&#39;version&#39;, version=&#39;%(prog)s v{}&#39;.format(__version__))

    subparsers = parser.add_subparsers(title=&#39;Sub-commands&#39;,
                                       description=&#39;To run this command, you must specify one of the functions listed&#39;
                                                   &#39;below next to the command. For more information on each function, &#39;
                                                   &#39;use -h next to the function name to call help document.&#39;,
                                       help=&#39;description&#39;,
                                       dest=&#39;function&#39;,
                                       metavar=&#39;command&#39;)

    input_str = &#34;input raw Bruker data&#34;
    input_dir_str = &#34;input directory that contains multiple raw Bruker data&#34;
    output_dir_str = &#34;output directory name&#34;
    output_fnm_str = &#34;output filename&#34;
    bids_opt = &#34;create a JSON file contains metadata according to BIDS recommendation&#34;

    info = subparsers.add_parser(&#34;info&#34;, help=&#39;Prints out the information of the internal contents in Bruker raw data&#39;)
    info.add_argument(&#34;input&#34;, help=input_str, type=str)

    gui = subparsers.add_parser(&#34;gui&#34;, help=&#39;Run GUI mode&#39;)
    nii = subparsers.add_parser(&#34;tonii&#34;, help=&#39;Convert a single raw Bruker data into NifTi file(s)&#39;)
    niiall = subparsers.add_parser(&#34;tonii_all&#34;, help=&#34;Convert All raw Bruker data located in the input directory&#34;)
    bids_helper = subparsers.add_parser(&#34;bids_helper&#34;, help=&#34;Creates a BIDS datasheet &#34;
                                                            &#34;for guiding BIDS data converting.&#34;)
    bids_convert = subparsers.add_parser(&#34;bids_convert&#34;, help=&#34;Convert ALL raw Bruker data located &#34;
                                                              &#34;in the input directory based on the BIDS datasheet&#34;)

    gui.add_argument(&#34;-i&#34;, &#34;--input&#34;, help=input_str, type=str, default=None)
    gui.add_argument(&#34;-o&#34;, &#34;--output&#34;, help=output_dir_str, type=str, default=None)

    nii.add_argument(&#34;input&#34;, help=input_str, type=str)
    nii.add_argument(&#34;-b&#34;, &#34;--bids&#34;, help=bids_opt, action=&#39;store_true&#39;)
    nii.add_argument(&#34;-o&#34;, &#34;--output&#34;, help=output_fnm_str, type=str, default=False)
    nii.add_argument(&#34;-r&#34;, &#34;--recoid&#34;, help=&#34;RECO ID&#34;, type=int, default=1)
    nii.add_argument(&#34;-s&#34;, &#34;--scanid&#34;, help=&#34;Scan ID&#34;, type=str)

    niiall.add_argument(&#34;input&#34;, help=input_dir_str, type=str)
    niiall.add_argument(&#34;-o&#34;, &#34;--output&#34;, help=output_dir_str, type=str)
    niiall.add_argument(&#34;-b&#34;, &#34;--bids&#34;, help=bids_opt, action=&#39;store_true&#39;)

    bids_helper.add_argument(&#34;input&#34;, help=input_dir_str, type=str)
    bids_helper.add_argument(&#34;output&#34;, help=&#34;output BIDS datasheet filename (.xlsx)&#34;, type=str)
    bids_helper.add_argument(&#34;-j&#34;, &#34;--json&#34;, help=&#34;create JSON syntax template for &#34;
                                                  &#34;parsing metadata from the header&#34;, action=&#39;store_true&#39;)

    bids_convert.add_argument(&#34;input&#34;, help=input_dir_str, type=str)
    bids_convert.add_argument(&#34;datasheet&#34;, help=&#34;input BIDS datahseet filename&#34;, type=str)
    bids_convert.add_argument(&#34;-j&#34;, &#34;--json&#34;, help=&#34;input JSON syntax template filename&#34;, type=str, default=False)
    bids_convert.add_argument(&#34;-o&#34;, &#34;--output&#34;, help=output_dir_str, type=str, default=False)

    args = parser.parse_args()

    if args.function == &#39;info&#39;:
        path = args.input
        if any([os.path.isdir(path), (&#39;zip&#39; in path), (&#39;PvDataset&#39; in path)]):
            study = BrukerLoader(path)
            study.info()
        else:
            list_path = [d for d in os.listdir(&#39;.&#39;) if (any([os.path.isdir(d),
                                                             (&#39;zip&#39; in d),
                                                             (&#39;PvDataset&#39; in d)]) and re.search(path, d, re.IGNORECASE))]
            for p in list_path:
                study = BrukerLoader(p)
                study.info()

    elif args.function == &#39;gui&#39;:
        ipath = args.input
        opath = args.output
        from ..ui.main_win import MainWindow
        root = MainWindow()
        if ipath != None:
            root._path = ipath
            root._extend_layout()
            root._load_dataset()
        if opath != None:
            root._output = opath
        root.mainloop()

    elif args.function == &#39;tonii&#39;:
        path = args.input
        scan_id = args.scanid
        reco_id = args.recoid
        study = BrukerLoader(path)
        if args.output:
            output = args.output
        else:
            output = &#39;{}_{}&#39;.format(study._pvobj.subj_id,study._pvobj.study_id)
        if scan_id:
            output_fname = &#39;{}-{}-{}&#39;.format(output, scan_id, reco_id)
            try:
                study.save_as(scan_id, reco_id, output_fname)
                if args.bids:
                    study.save_json(scan_id, reco_id, output_fname)
                print(&#39;NifTi file is generated... [{}]&#39;.format(output_fname))
            except Exception as e:
                print(&#39;[Warning]::{}&#39;.format(e))
        else:
            for scan_id, recos in study._pvobj.avail_reco_id.items():
                for reco_id in recos:
                    output_fname = &#39;{}-{}-{}&#39;.format(output, str(scan_id).zfill(2), reco_id)
                    try:
                        study.save_as(scan_id, reco_id, output_fname)
                        if args.bids:
                            study.save_json(scan_id, reco_id, output_fname)
                        print(&#39;NifTi file is genetared... [{}]&#39;.format(output_fname))
                    except Exception as e:
                        print(&#39;[Warning]::{}&#39;.format(e))

    elif args.function == &#39;tonii_all&#39;:
        path = args.input
        from os.path import join as opj, isdir, isfile
        list_of_raw = sorted([d for d in os.listdir(path) if isdir(opj(path, d)) \
                              or (isfile(opj(path, d)) and ((&#39;zip&#39; in d) or (&#39;PvDataset&#39; in d)))])
        base_path = &#39;Data&#39;
        try:
            os.mkdir(base_path)
        except:
            pass
        for raw in list_of_raw:
            sub_path = os.path.join(path, raw)
            study = BrukerLoader(sub_path)
            if len(study._pvobj.avail_scan_id):
                subj_path = os.path.join(base_path, &#39;sub-{}&#39;.format(study._pvobj.subj_id))
                try:
                    os.mkdir(subj_path)
                except OSError:
                    pass
                else:
                    raise UnexpectedError
                sess_path = os.path.join(subj_path, &#39;ses-{}&#39;.format(study._pvobj.study_id))
                try:
                    os.mkdir(sess_path)
                except OSError:
                    pass
                else:
                    raise UnexpectedError
                for scan_id, recos in study._pvobj.avail_reco_id.items():
                    method = study._pvobj._method[scan_id].parameters[&#39;Method&#39;]
                    if re.search(&#39;epi&#39;, method, re.IGNORECASE) and not re.search(&#39;dti&#39;, method, re.IGNORECASE):
                        output_path = os.path.join(sess_path, &#39;func&#39;)
                    elif re.search(&#39;dti&#39;, method, re.IGNORECASE):
                        output_path = os.path.join(sess_path, &#39;dwi&#39;)
                    elif re.search(&#39;flash&#39;, method, re.IGNORECASE) or re.search(&#39;rare&#39;, method, re.IGNORECASE):
                        output_path = os.path.join(sess_path, &#39;anat&#39;)
                    else:
                        output_path = os.path.join(sess_path, &#39;etc&#39;)
                    try:
                        os.mkdir(output_path)
                    except OSError:
                        pass
                    else:
                        raise UnexpectedError
                    filename = &#39;sub-{}_ses-{}_{}&#39;.format(study._pvobj.subj_id, study._pvobj.study_id,
                                                         str(scan_id).zfill(2))
                    for reco_id in recos:
                        output_fname = os.path.join(output_path, &#39;{}_reco-{}&#39;.format(filename,
                                                                                     str(reco_id).zfill(2)))
                        try:
                            study.save_as(scan_id, reco_id, output_fname)
                            if args.bids:
                                study.save_json(scan_id, reco_id, output_fname)
                            if re.search(&#39;dti&#39;, method, re.IGNORECASE):
                                study.save_bdata(scan_id, reco_id, output_fname)
                        except Exception as e:
                            print(e)
                print(f&#39;{raw} is converted...&#39;)
            else:
                print(f&#39;{raw} is empty...&#39;)

    elif args.function == &#39;bids_helper&#39;:
        import pandas as pd
        path = os.path.abspath(args.input)
        ds_output = os.path.abspath(args.output)

        make_json = args.json
        if not ds_output.endswith(&#39;.xlsx&#39;):
            # to prevent pandas ValueError in case user does not provide valid file extension.
            output = f&#39;{ds_output}.xlsx&#39;
        else:
            output = ds_output

        Headers = [&#39;RawData&#39;, &#39;SubjID&#39;, &#39;SessID&#39;, &#39;ScanID&#39;, &#39;RecoID&#39;, &#39;DataType&#39;,
                   &#39;task&#39;, &#39;acq&#39;, &#39;ce&#39;, &#39;rec&#39;, &#39;dir&#39;, &#39;run&#39;, &#39;modality&#39;, &#39;Start&#39;, &#39;End&#39;]
        df = pd.DataFrame(columns=Headers)

        for dname in sorted(os.listdir(path)):
            dpath = os.path.join(path, dname)
            try:
                dset = BrukerLoader(dpath)
            except:
                dset = None

            if dset is not None:
                if dset.is_pvdataset:
                    pvobj = dset.pvobj

                    rawdata = pvobj.path
                    subj_id = pvobj.subj_id
                    sess_id = pvobj.session_id

                    for scan_id, recos in pvobj.avail_reco_id.items():
                        for reco_id in recos:
                            visu_pars = dset.get_visu_pars(scan_id, reco_id)
                            if dset._get_dim_info(visu_pars)[1] == &#39;spatial_only&#39;:
                                num_spack = dset._get_slice_info(visu_pars)[&#39;num_slice_packs&#39;]

                                if num_spack != 3:  # excluding localizer
                                    method = dset.get_method(scan_id).parameters[&#39;Method&#39;]
                                    if re.search(&#39;epi&#39;, method, re.IGNORECASE) and not re.search(&#39;dti&#39;, method, re.IGNORECASE):
                                        datatype = &#39;func&#39;
                                    elif re.search(&#39;dti&#39;, method, re.IGNORECASE):
                                        datatype = &#39;dwi&#39;
                                    elif re.search(&#39;flash&#39;, method, re.IGNORECASE) or re.search(&#39;rare&#39;, method, re.IGNORECASE):
                                        datatype = &#39;anat&#39;
                                    elif re.search(&#39;fieldmap&#39;, method, re.IGNORECASE):
                                        datatype = &#39;fmap&#39;
                                    else:
                                        datatype = &#39;etc&#39;

                                    item = dict(zip(Headers, [rawdata, subj_id, sess_id, scan_id, reco_id, datatype]))
                                    if datatype == &#39;fmap&#39;:
                                        for m, s, e in [[&#39;fieldmap&#39;, 0, 1], [&#39;magnitude&#39;, 1, 2]]:
                                            item[&#39;modality&#39;] = m
                                            item[&#39;Start&#39;] = s
                                            item[&#39;End&#39;] = e
                                            df = df.append(item, ignore_index=True)
                                    else:
                                        df = df.append(item, ignore_index=True)
        df.to_excel(output, index=None)

        if make_json:
            fname = output[:-5]
            json_fname = f&#39;{fname}.json&#39;
            print(f&#39;Creating JSON syntax template for parsing the BIDS required metadata &#39;
                  f&#39;(BIDS v{_supporting_bids_ver}): {json_fname}&#39;)
            with open(json_fname, &#39;w&#39;) as f:
                import json
                from ..lib.reference import COMMON_META_REF, FMRI_META_REF, FIELDMAP_META_REF
                ref_dict = dict(common=COMMON_META_REF,
                                func=FMRI_META_REF,
                                fmap=FIELDMAP_META_REF)
                json.dump(ref_dict, f, indent=4)

        print(&#39;[Important notice] The function helps to minimize the BIDS organization but does not guarantee that &#39;
              &#39;the dataset always meets the BIDS requirements. &#39;
              &#39;Therefore, after converting your data, we recommend validating &#39;
              &#39;your dataset using an official BIDS validator.&#39;)

    elif args.function == &#39;bids_convert&#39;:
        import pandas as pd
        import numpy as np
        import json
        import datetime
        from ..lib.utils import build_bids_json, bids_validation

        pd.options.mode.chained_assignment = None
        path = args.input
        datasheet = args.datasheet
        output = args.output
        df = pd.read_excel(datasheet, dtype={&#39;SubjID&#39;: str, &#39;SessID&#39;: str, &#39;run&#39;: str})
        json_fname = args.json

        # check if the project is multi-session
        if all(pd.isnull(df[&#39;SessID&#39;])):
            # SessID was removed
            multi_session = False
        else:
            num_session = len(list(set(df[&#39;SessID&#39;])))
            if num_session &gt; 1:
                multi_session = True
            else:
                multi_session = False

        if not output:
            root_path = os.path.abspath(os.path.join(os.path.curdir, &#39;Data&#39;))
        else:
            root_path = output

        mkdir(root_path)

        # prepare the required file for converted BIDS dataset
        data_des = &#39;dataset_description.json&#39;
        readme = &#39;README&#39;
        if not os.path.exists(data_des):
            with open(os.path.join(root_path, &#39;dataset_description.json&#39;), &#39;w&#39;) as f:
                from ..lib.reference import DATASET_DESC_REF
                json.dump(DATASET_DESC_REF, f, indent=4)
        if not os.path.exists(readme):
            with open(os.path.join(root_path, readme), &#39;w&#39;) as f:
                f.write(f&#39;This dataset has been converted using BrkRaw (v{__version__}) at {datetime.datetime.now()}.\n&#39;)
                f.write(&#39;## How to cite?\n - https://doi.org/10.5281/zenodo.3818615\n&#39;)

        print(&#39;Inspect input BIDS datasheet...&#39;)
        for dname in sorted(os.listdir(path)):
            dpath = os.path.join(path, dname)
            try:
                dset = BrukerLoader(dpath)
                if dset.is_pvdataset:
                    pvobj = dset.pvobj
                    rawdata = pvobj.path
                    filtered_dset = df[df[&#39;RawData&#39;].isin([rawdata])].reset_index()
                    filtered_dset.loc[:, &#39;FileName&#39;] = [np.nan] * len(filtered_dset)
                    filtered_dset.loc[:, &#39;Dir&#39;] = [np.nan] * len(filtered_dset)

                    if len(filtered_dset):
                        subj_id = list(set(filtered_dset[&#39;SubjID&#39;]))[0]
                        subj_code = f&#39;sub-{subj_id}&#39;

                        for i, row in filtered_dset.iterrows():
                            if multi_session:
                                # If multi-session, make session dir
                                sess_code = f&#39;ses-{row.SessID}&#39;
                                subj_path = os.path.join(root_path, subj_code)
                                mkdir(subj_path)
                                subj_path = os.path.join(subj_path, sess_code)
                                mkdir(subj_path)
                                # add session info to filename as well
                                fname = f&#39;{subj_code}_{sess_code}&#39;
                            else:
                                subj_path = os.path.join(root_path, subj_code)
                                mkdir(subj_path)
                                fname = f&#39;{subj_code}&#39;

                            datatype = row.DataType
                            dtype_path = os.path.join(subj_path, datatype)
                            mkdir(dtype_path)

                            if pd.notnull(row.task):
                                if bids_validation(df, i, &#39;task&#39;, row.task, 10):
                                    fname = f&#39;{fname}_task-{row.task}&#39;
                            if pd.notnull(row.acq):
                                if bids_validation(df, i, &#39;acq&#39;, row.acq, 10):
                                    fname = f&#39;{fname}_acq-{row.acq}&#39;
                            if pd.notnull(row.ce):
                                if bids_validation(df, i, &#39;ce&#39;, row.ce, 5):
                                    fname = f&#39;{fname}_ce-{row.ce}&#39;
                            if pd.notnull(row.dir):
                                if bids_validation(df, i, &#39;dir&#39;, row.dir, 2):
                                    fname = f&#39;{fname}_dir-{row.dir}&#39;
                            if pd.notnull(row.rec):
                                if bids_validation(df, i, &#39;rec&#39;, row.rec, 2):
                                    fname = f&#39;{fname}_rec-{row.rec}&#39;
                            filtered_dset.loc[i, &#39;FileName&#39;] = fname
                            filtered_dset.loc[i, &#39;Dir&#39;] = dtype_path
                            if pd.isnull(row.modality):
                                method = dset.get_method(row.ScanID).parameters[&#39;Method&#39;]
                                if row.DataType == &#39;anat&#39;:
                                    if re.search(&#39;flash&#39;, method, re.IGNORECASE):
                                        modality = &#39;FLASH&#39;
                                    elif re.search(&#39;rare&#39;, method, re.IGNORECASE):
                                        modality = &#39;T2w&#39;
                                    else:
                                        modality = &#39;{}&#39;.format(method.split(&#39;:&#39;)[-1])
                                else:
                                    modality = &#39;{}&#39;.format(method.split(&#39;:&#39;)[-1])
                                filtered_dset.loc[i, &#39;modality&#39;] = modality
                            else:
                                bids_validation(df, i, &#39;modality&#39;, row.modality, 10, dtype=str)

                        list_tested_fn = []
                        # Converting data according to the updated sheet
                        print(f&#39;Converting {dname}...&#39;)

                        for i, row in filtered_dset.iterrows():
                            temp_fname = f&#39;{row.FileName}_{row.modality}&#39;
                            if temp_fname not in list_tested_fn:
                                # filter the DataFrame that has same filename (updated without run)
                                fn_filter = filtered_dset.loc[:, &#39;FileName&#39;].isin([row.FileName])
                                fn_df = filtered_dset[fn_filter].reset_index(drop=True)

                                # filter specific modality from above DataFrame
                                md_filter = fn_df.loc[:, &#39;modality&#39;].isin([row.modality])
                                md_df = fn_df[md_filter].reset_index(drop=True)

                                if len(md_df) &gt; 1:
                                    conflict_tested = []
                                    for j, sub_row in md_df.iterrows():
                                        if pd.isnull(sub_row.run):
                                            fname = f&#39;{sub_row.FileName}_run-{str(j+1).zfill(2)}&#39;
                                        else:
                                            _ = bids_validation(df, i, &#39;run&#39;, sub_row.run, 3, dtype=int)
                                            fname = f&#39;{sub_row.FileName}_run-{str(sub_row.run).zfill(2)}&#39;
                                        if fname in conflict_tested:
                                            raise ValueConflictInField(f&#39;ScanID:[{sub_row.ScanID}] Conflict error. &#39;
                                                                       &#39;The [run] index value must be unique &#39;
                                                                       &#39;among the scans with the same modality.&#39;)
                                        else:
                                            conflict_tested.append(fname)
                                        build_bids_json(dset, sub_row, fname, json_fname)
                                else:
                                    fname = f&#39;{row.FileName}&#39;
                                    build_bids_json(dset, row, fname, json_fname)
                                list_tested_fn.append(temp_fname)
                        print(&#39;...Done.&#39;)
            except FileNotValidError:
                pass
    else:
        parser.print_help()</code></pre>
</details>
</dd>
<dt id="brkraw.scripts.brkraw.mkdir"><code class="name flex">
<span>def <span class="ident">mkdir</span></span>(<span>path)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mkdir(path):
    try:
        os.stat(path)
    except FileNotFoundError or OSError:
        os.makedirs(path)
    except:
        raise UnexpectedError</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="brkraw.scripts" href="index.html">brkraw.scripts</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="brkraw.scripts.brkraw.main" href="#brkraw.scripts.brkraw.main">main</a></code></li>
<li><code><a title="brkraw.scripts.brkraw.mkdir" href="#brkraw.scripts.brkraw.mkdir">mkdir</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>